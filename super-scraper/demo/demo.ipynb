{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticToolsParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate 5 different versions of\n",
    "    the given user questions to retrieve relevant data from the internet. In order to this well, you will need to \n",
    "    think like a political consultant incharge of running a campaign. By generating multiple perspectives on the user question, \n",
    "    your goal is to help the user get a better answer with a full picture. Provide these alternative questions separated by newlines. \n",
    "    Original question: {question}\"\"\"\n",
    "decomp_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", template),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "query_analyzer = (decomp_prompt | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "sub_queries = query_analyzer.invoke({\"question\":question})\n",
    "sub_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import crewai\n",
    "\n",
    "# define an agent\n",
    "    # duckduckgo\n",
    "    # wikipedia agent\n",
    "    # web scraper focused on newspapers\n",
    "\n",
    "# for loop\n",
    "# each query \n",
    "# crew = Crew(\n",
    "#   agents=[researcher, manager],\n",
    "#   tasks=[task1, task2],\n",
    "#   process=Process.sequential,\n",
    "#   verbose=2\n",
    "# )\n",
    "# result = crew.kickoff() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai.task import TaskOutput\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain.agents import load_tools\n",
    "from langchain_community.tools.wikidata.tool import WikidataAPIWrapper, WikidataQueryRun\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from crewai_tools import (\n",
    "    WebsiteSearchTool,\n",
    "    ScrapeWebsiteTool\n",
    ")\n",
    "\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "# wikipedia_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "# wikidata_tool = WikidataQueryRun(api_wrapper=WikidataAPIWrapper())\n",
    "website_search_tool = WebsiteSearchTool()\n",
    "scrape_tool = ScrapeWebsiteTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agents\n",
    "search_agent = Agent(\n",
    "    role='Researcher',\n",
    "    goal='Find up-to-date information and news articles',\n",
    "    backstory=\"\"\"An expert researcher that is able to search the web to find\n",
    "        information on anything. You don't give up until all the research is complete.\"\"\",\n",
    "    tools=[search_tool, website_search_tool],\n",
    "    # verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agents\n",
    "scraper_agent = Agent(\n",
    "    role='Scraper',\n",
    "    goal='Find the most relevant websites related to search query.',\n",
    "    backstory=\"\"\"An expert web-scraper, you don't stop trying until you get the information you are looking for. If you fail trying to scrape a website, you don't give up.\"\"\",\n",
    "    tools=[scrape_tool],\n",
    "    # verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agents\n",
    "writer = Agent(\n",
    "    role='Report & Memo Writer',\n",
    "    goal='With the provided information, generate a long and thorough report.',\n",
    "    backstory=\"\"\"You are a Political Campaign manager with excellent writing skills. Given all the research data, you are able to summarize, draw conclusions, and then generate a report for the rest of the team to understand the question.\"\"\",\n",
    "    tools=[scrape_tool],\n",
    "    # verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_task_list = []\n",
    "\n",
    "for query in sub_queries:\n",
    "    # iterate through queries, define search and scrape task\n",
    "    search_task = Task(\n",
    "        description=f\"\"\"Search the web to gather information on {query} using trusted sources such as \n",
    "        \"\"\",\n",
    "        expected_output=\"A list of facts\",\n",
    "        agent= search_agent,\n",
    "        tools=[search_tool, website_search_tool]\n",
    "    )\n",
    "\n",
    "    scrape_task = Task(\n",
    "        description=\"\"\"Take the data provided and write a detailed report to help an election campaign come up with a plan to win.  \n",
    "            \"\"\",\n",
    "        agent=scraper_agent,\n",
    "        expected_output=\"A report on the city\",\n",
    "        tools=[scrape_tool],\n",
    "    )\n",
    "    # once tasks are defined for each query, append to task_list\n",
    "    query_task_list.append(search_task, scrape_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final task for summary/report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the crew\n",
    "# Instantiate your crew with a sequential process\n",
    "crew = Crew(\n",
    "  agents=[], # add in the agents\n",
    "  tasks=[], #add in the tasks \n",
    "  process=Process.sequential,\n",
    "  verbose=2\n",
    ")\n",
    "result = crew.kickoff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](\"./76E82B5C-6B16-4566-8554-6B1B7D4C33C7_1_105_c.jpeg\")\n",
    "\n",
    "![rag-flowchart-langchain](rag-flowchart.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Get query\n",
    "2. Expand Query using Decomposition\n",
    "3. Pass all sub-queries to crew ai agents\n",
    "4. Review all info\n",
    "5. create a memo\n",
    "\n",
    "Agents\n",
    "1. scraper\n",
    "2. search duck duck go\n",
    "3. Compiler/Reviewer - request more info from agents 1 and 2?\n",
    "4. Memo Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "load_dotenv()\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_model = os.getenv(\"OPENAI_MODEL_NAME\")\n",
    "os.environ[\"OPENAI_MODEL_NAME\"]= openai_model\n",
    "os.environ[\"OPENAI_API_KEY\"]= openai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the political landscape of Gainesville, Fl?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition Step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
